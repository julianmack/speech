{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of RNN-T loss implementations\n",
    "Note: this will only run inside the awni-speech docker image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/speech/libs\n"
     ]
    }
   ],
   "source": [
    "cd '/home/ubuntu/speech/libs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transducer.functions.transducer as awni_transducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warprnnt_pytorch import RNNTLoss as WarpRNNTLoss\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_awni = [log_probs, y_flat, lengths, label_lengths]\n",
    "# inputs_warp =  [logits, y_mat, logit_lens, y_lens]\n",
    "B = 2\n",
    "T = 4\n",
    "K = 5\n",
    "U = 2\n",
    "logits = torch.randn((B, T, U + 1, K + 1))\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "probs = log_probs.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(log_probs)\n",
    "# print(logits)\n",
    "# print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_collate(labels):\n",
    "    # Doesn't matter what we pad the end with\n",
    "    # since it will be ignored.\n",
    "    batch_size = len(labels)\n",
    "    end_tok = labels[0][-1]\n",
    "    max_len = max(len(l) for l in labels)\n",
    "    cat_labels = np.full((batch_size, max_len),\n",
    "                    fill_value=end_tok, dtype=np.int64)\n",
    "    for e, l in enumerate(labels):\n",
    "        cat_labels[e, :len(l)] = l\n",
    "    labels = torch.LongTensor(cat_labels)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [torch.IntTensor([0, 1]), torch.IntTensor([1, 0])]\n",
    "y_flat = torch.IntTensor([l for label in y for l in label])\n",
    "y_flat\n",
    "y_mat = label_collate(y).int()\n",
    "x_lens = torch.IntTensor([4, 2]) #T=3\n",
    "y_lens = torch.IntTensor([len(l) for l in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_awni = [log_probs, y_flat, x_lens, y_lens]\n",
    "inputs_warp =  [logits, y_mat, x_lens, y_lens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False\n",
    "if gpu:\n",
    "    inputs_awni = [x.cpu() for x in inputs_awni] #this has no gpu implementation\n",
    "    inputs_warp = [x.cuda() for x in inputs_warp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_label = K\n",
    "awni_loss = awni_transducer.TransducerLoss(blank_label=blank_label)\n",
    "warp_loss = WarpRNNTLoss(blank=blank_label)\n",
    "# awni_loss = awni_transducer.TransducerLoss()\n",
    "# warp_loss = WarpRNNTLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.0178])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "awni_loss(*inputs_awni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 6])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n",
      "torch.Size([2])\n",
      "tensor([4, 2], dtype=torch.int32) tensor([2, 2], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([7.0178])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in inputs_warp:\n",
    "    print(x.shape)\n",
    "print(inputs_warp[-2], inputs_warp[-1])\n",
    "\n",
    "warp_loss(*inputs_warp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "* warp_loss and awni_loss give the same value. It is obviously possible they are both wrong (I think warp-rnnt was based on awni's implementation).\n",
    "* warp_loss appears to give the same values reagrdless of whether logits or log_probs are passed. This is quite weird - there must be a check in the implementation that determines whether log_probs or logits have been passed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same(inp1, inp2):\n",
    "    epsilon = 1e-5\n",
    "    val1 = warp_loss(*inp1)\n",
    "    val2 = warp_loss(*inp2)\n",
    "    assert (val1.item() - val2.item()) < epsilon, f\"log probs and logits do not give the same values. i.e {val1.item()} != {val2.item()}\"\n",
    "    print(f\"Losses are the same since {val1.item()} == {val2.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses are the same since 6.881084442138672 == 6.881083965301514\n",
      "Losses are the same since 6.881107330322266 == 6.881138801574707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4129, -1.6022, -0.9453,  0.5857,  0.6219, -0.7563],\n",
       "          [ 2.1744,  2.4167, -0.1291, -0.3900, -0.4496, -0.7070],\n",
       "          [ 0.3734,  0.0791, -0.8818, -0.7426, -0.8053, -1.5643]],\n",
       "\n",
       "         [[ 0.0635, -0.8184,  1.1282, -1.3565,  0.3035,  0.3024],\n",
       "          [-1.2073,  0.9886, -0.6912, -1.5064, -0.2042,  0.5708],\n",
       "          [-0.2789, -0.7848, -1.1220,  0.6504,  0.0577,  0.4951]],\n",
       "\n",
       "         [[ 0.4040,  0.3438,  0.8030,  0.4080, -1.1770,  1.1413],\n",
       "          [ 1.5867,  1.8462, -0.8938, -0.6556,  1.0368,  0.6939],\n",
       "          [ 0.6407,  0.4395,  0.0393,  1.6467,  0.4994,  0.6421]],\n",
       "\n",
       "         [[ 0.9332,  0.5397, -0.9760, -1.7922, -1.2370, -1.3334],\n",
       "          [-1.6242, -1.4349,  0.0672, -0.2375,  0.7944,  1.1286],\n",
       "          [ 0.0827, -0.3559, -1.1331, -0.0338, -0.9112,  1.2635]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1498, -0.3466, -2.0149, -0.2680, -0.4186, -0.9724],\n",
       "          [ 0.9700, -0.7038,  0.3109,  0.5435,  1.3241,  0.1239],\n",
       "          [ 1.0267,  2.0639, -0.3451,  0.8338, -0.1505,  0.3615]],\n",
       "\n",
       "         [[ 1.8789,  0.9867,  0.6439, -0.2017, -1.0351, -1.0573],\n",
       "          [ 0.3627,  0.5018, -2.1326, -0.1932,  0.5846,  0.3042],\n",
       "          [-1.5803, -0.9224,  0.3271, -0.6209,  0.0215, -0.0251]],\n",
       "\n",
       "         [[ 1.0518,  0.1175, -0.9022, -0.0348, -0.0510,  2.2759],\n",
       "          [-0.6593, -0.4156,  1.0388, -1.2538, -0.4287, -0.4740],\n",
       "          [-1.3371, -0.4773, -1.3910, -0.5192,  0.2723,  0.2844]],\n",
       "\n",
       "         [[ 0.7747, -0.0460,  0.6910,  0.7801, -0.4812,  2.2327],\n",
       "          [ 0.6740, -0.8320,  0.5083,  1.1483,  2.6965,  1.9126],\n",
       "          [-0.3681, -0.5605,  0.8157, -1.4788, -1.0521,  0.6936]]]])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn((B, T, U + 1, K + 1))\n",
    "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "inputs_warp1 =  [logits, y_mat, x_lens, y_lens]\n",
    "inputs_warp2 = [log_probs, y_mat, x_lens, y_lens]\n",
    "\n",
    "assert_same(inputs_warp1, inputs_warp2)\n",
    "#perhaps the check is seeing if all the values are negative?\n",
    "\n",
    "logits2 = logits.clone()\n",
    "logits2 -= 1000 #make all negative\n",
    "log_probs2 = torch.nn.functional.log_softmax(logits2, dim=-1)\n",
    "\n",
    "inputs_warp3 = [log_probs2, y_mat, x_lens, y_lens]\n",
    "inputs_warp4 =  [logits2, y_mat, x_lens, y_lens]\n",
    "\n",
    "assert_same(inputs_warp3, inputs_warp4)\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logits vs log_probs conclusion\n",
    "\n",
    "Ok so as expected, the implementation treats the input as log_probs if it is all negative (or all positive by the looks of it) but if the values are mixed signs it assumes they are logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked example 1\n",
    "Consider three timesteps with perfectly trained model and exact alignments such that correct output should be:  \n",
    "* a @ T=1\n",
    "* b @ T=2\n",
    "* blank @ T=3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9.9998e-01, 1.0000e-05, 1.0000e-05],\n",
       "          [1.0000e-05, 1.0000e-05, 9.9998e-01],\n",
       "          [1.0000e-05, 1.0000e-05, 9.9998e-01]],\n",
       "\n",
       "         [[1.0000e-05, 1.0000e-05, 9.9998e-01],\n",
       "          [1.0000e-05, 9.9998e-01, 1.0000e-05],\n",
       "          [1.0000e-05, 1.0000e-05, 9.9998e-01]],\n",
       "\n",
       "         [[1.0000e-05, 1.0000e-05, 9.9998e-01],\n",
       "          [1.0000e-05, 1.0000e-05, 9.9998e-01],\n",
       "          [1.0000e-05, 1.0000e-05, 9.9998e-01]]]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_t1 = torch.tensor([[1., 0, 0], [0, 0, 1], [0, 0, 1]])\n",
    "probs_t2 = torch.tensor([[0., 0, 1], [0, 1, 0], [0, 0, 1]])\n",
    "probs_t3 = torch.tensor([[0, 0, 1.], [0, 0, 1], [0, 0, 1]])\n",
    "\n",
    "probs = [probs_t1, probs_t2, probs_t3]\n",
    "probs = [x.unsqueeze(0) for x in probs]\n",
    "probs = torch.cat(probs, dim=0)\n",
    "probs = probs.unsqueeze(0).float()\n",
    "\n",
    "EPS = 0.00001 #Non-zero value of other \n",
    "r = 2 * (1 - 3 * EPS)\n",
    "probs *= r\n",
    "probs += 2 * EPS\n",
    "probs /= 2.0\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -0.0000, -11.5129, -11.5129],\n",
       "          [-11.5129, -11.5129,  -0.0000],\n",
       "          [-11.5129, -11.5129,  -0.0000]],\n",
       "\n",
       "         [[-11.5129, -11.5129,  -0.0000],\n",
       "          [-11.5129,  -0.0000, -11.5129],\n",
       "          [-11.5129, -11.5129,  -0.0000]],\n",
       "\n",
       "         [[-11.5129, -11.5129,  -0.0000],\n",
       "          [-11.5129, -11.5129,  -0.0000],\n",
       "          [-11.5129, -11.5129,  -0.0000]]]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs = probs.log()\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [torch.IntTensor([0, 1])]\n",
    "y_flat = torch.IntTensor([l for label in y for l in label])\n",
    "y_mat = label_collate(y).int()\n",
    "x_lens = torch.IntTensor([3]) #T=3\n",
    "y_lens = torch.IntTensor([len(l) for l in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0001])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_label = 2\n",
    "inputs_awni = [log_probs, y_flat, x_lens, y_lens]\n",
    "awni_loss = awni_transducer.TransducerLoss(blank_label=blank_label)\n",
    "awni_loss(*inputs_awni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0001])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warp_loss = WarpRNNTLoss(blank=blank_label)\n",
    "inputs_warp =  [log_probs, y_mat, x_lens, y_lens]\n",
    "warp_loss(*inputs_warp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 3])\n",
      "torch.Size([1, 2])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for x in inputs_warp:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is behaving as expected - i.e. monotonically increasing from 0 as EPS increases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked example 2\n",
    "Consider untrained model (all probabilities equally likely). Work out expected loss. \n",
    "* Alphabet = {a, blank}\n",
    "* i.e. K = 1 (hence all probabilities are 0.5)\n",
    "* T = 2\n",
    "* There are two paths through the graph: (blank, a) or (a, blank)\n",
    "* Each of these requires x3 transitions (i.e. must append blank at end)\n",
    "\n",
    "* expected total: 0.5 ** 3 x 2 = 0.25\n",
    "\n",
    "* LOSS = - ln(0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED_LOSS = - torch.log(torch.tensor([1 / 4.]))\n",
    "probs = torch.ones((1, 2, 2, 2)) * 0.5\n",
    "log_probs = probs.log()\n",
    "y = [torch.IntTensor([0])]\n",
    "y_flat = torch.IntTensor([l for label in y for l in label])\n",
    "y_mat = label_collate(y).int()\n",
    "x_lens = torch.IntTensor([2]) #T=3\n",
    "y_lens = torch.IntTensor([len(l) for l in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_label = 1\n",
    "inputs_awni = [log_probs, y_flat, x_lens, y_lens]\n",
    "awni_loss = awni_transducer.TransducerLoss(blank_label=blank_label)\n",
    "loss1 = awni_loss(*inputs_awni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "warp_loss = WarpRNNTLoss(blank=blank_label)\n",
    "inputs_warp =  [log_probs, y_mat, x_lens, y_lens]\n",
    "loss2 = warp_loss(*inputs_warp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3863]) tensor([1.3863])\n"
     ]
    }
   ],
   "source": [
    "print(loss1, loss2)\n",
    "assert torch.allclose(loss1, EXPECTED_LOSS), f\"loss={loss1} != {EXPECTED_LOSS}\"\n",
    "assert torch.allclose(loss2, EXPECTED_LOSS), f\"loss={loss2} != {EXPECTED_LOSS}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
